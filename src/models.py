import torch
import torch.nn as nn
from torch_geometric.nn import GCNConv
import torch.nn.functional as F
import pytorch_lightning as pl
from torch.optim import Adam

# for now using an MLP as encoder and decoder
class VariationalEncoder(nn.Module):
    def __init__(self, input_dim: int, hidden_dim: int, latent_dim: int):
        super().__init__()
        self.linear1 = nn.Linear(input_dim, hidden_dim)
        # self.linear2 = nn.Linear(hidden_dim, hidden_dim)
        self.linear_mu = nn.Linear(hidden_dim, latent_dim)
        self.linear_sigma = nn.Linear(hidden_dim, latent_dim)

        self.N = torch.distributions.Normal(0, 1)
        # sample on the gpu
        self.N.loc = self.N.loc.cuda()
        self.N.scale = self.N.scale.cuda()
        self.kl = 0
    
    def forward(self, x):
        x = F.relu(self.linear1(x))
        # x = F.relu(self.linear2(x))
        mu = self.linear_mu(x)
        sigma = torch.exp(self.linear_sigma(x))

        z = mu + sigma*self.N.sample(mu.shape)
        # print("mu:", mu)
        # print("sigma:", sigma)
        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()
        return z
    
    def forward_det(self, x):
        """
        Returns the deterministic mean of the gaussian generated by the sample
        """
        x = F.relu(self.linear1(x))
        x = F.relu(self.linear2(x))
        mu = self.linear_mu(x)
        return mu

class Decoder(nn.Module):
    def __init__(self, latent_dim:int, hidden_dim: int, output_dim:int):
        super().__init__()
        self.linear1 = nn.Linear(latent_dim, hidden_dim)
        self.linear2 = nn.Linear(hidden_dim, hidden_dim)
        self.linear3 = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        x = F.relu(self.linear1(x))
        x = F.relu(self.linear2(x))
        return self.linear3(x)


class VAE(pl.LightningModule):
    def __init__(self, input_dim: int, latent_dim: int):
        super().__init__()
        self.save_hyperparameters()

        self.hidden_dim = (input_dim + latent_dim)//2
        self.encoder = VariationalEncoder(input_dim, self.hidden_dim, latent_dim)
        self.decoder = Decoder(latent_dim, self.hidden_dim, input_dim)

    def forward(self, x):
        z = self.encoder(x)
        x_hat = self.decoder(z)
        return x_hat

    def get_latent(self, x):
        z = self.encoder.forward_det(x)
        return z

    def common_step(self, batch):
        x, _ =  batch
        x_hat = self(x)
        kl = self.encoder.kl
        loss = ((x - x_hat)**2).sum()

        # print("kl:", kl)
        # print("loss:",loss)
        return loss, kl
    
    def training_step(self, batch, batch_idx):
        loss, kl = self.common_step(batch)
        self.log('training_loss', loss, on_epoch=True)
        self.log('training_kl', kl, on_epoch=True)
        losses.append(float(loss))
        kls.append(float(kl))
        return loss + kl

    def configure_optimizers(self):
        optimizer = Adam(self.parameters(), lr=1e-3)
        return optimizer


class GNN(pl.LightningModule):
    def __init__(self, in_channels: int, hidden_channels: int, out_channels: int):
        super().__init__()
        self.save_hyperparameters()
        self.conv1 = GCNConv(in_channels, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, out_channels)

    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:
        # x: Node feature matrix of shape [num_nodes, in_channels]
        # edge_index: Graph connectivity matrix of shape [2, num_edges]
        x = self.conv1(x, edge_index).relu()
        x = self.conv2(x, edge_index)
        return x
    
    def training_step(self, batch, batch_idx):
        # training_step defines the train loop. It is independent of forward
        x, y = batch
        out = self(x)
        loss =  F.mse_loss(out, y)
        self.log('training_loss', loss, on_epoch=True)
        return loss

    def validation_step(self, batch, batch_idx):
        loss = self.training_step(batch, batch_idx)
        self.log('validation_loss', loss, on_epoch=True)
        return loss

    def test_step(self, batch, batch_idx):
        x, y = batch
        out = self(x)
        loss =  F.mse_loss(out, y)
        r2 = self.r2score(out, y)
        self.log('test_loss', loss, on_epoch=True)
        self.log('r2_score', r2, on_epoch=True)
        return loss
    
    def predict_step(self, batch, batch_idx, dataloader_idx=0):
        return self(batch[0].float())
    
    def configure_optimizers(self):
        optimizer = Adam(self.parameters(), lr=1e-3)
        return optimizer

if __name__ == "__main__":
    # GNN(5, 10, 3)
    from datasets import ClimARTDataset
    from torch.utils.data import DataLoader
    from pytorch_lightning.loggers import TensorBoardLogger

    data_train = ClimARTDataset(normalize=True)
    print("data mean:", data_train.data[0].mean(dim=0))
    print("data std:", data_train.data[0].std(dim=0))
    print(data_train.data[0][0])
    train_loader = DataLoader(data_train, batch_size=128, num_workers=128, shuffle=True)
    vae = VAE(data_train.input_dim, 512)

    logger = TensorBoardLogger("./logs/", name="VAE", version="ClimART_train")
    trainer = pl.Trainer(devices=1, accelerator="gpu", max_epochs=30, log_every_n_steps=10, logger=logger)
    losses, kls = [], []
    trainer.fit(vae, train_loader)

    import matplotlib.pyplot as plt
    loss_line, kl_line = plt.plot(losses, "r", kls, "g")
    loss_line.set_label("reconstruction loss")
    kl_line.set_label("kl divergence")
    plt.legend()
    plt.savefig("VAE_losses_ClimART.png")

    

    